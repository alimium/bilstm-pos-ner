{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "**Assignment 2**\n",
    "\n",
    "*Ali Mehrabi - 9912045*\n",
    "\n",
    "Part-of-Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import block\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import lightning as pl\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "np.random.seed(43)\n",
    "sgen = torch.manual_seed(43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw = pd.read_json(\"./data/POS/pos_train.json\")\n",
    "val_raw = pd.read_json(\"./data/POS/pos_val.json\")\n",
    "test_raw = pd.read_json(\"./data/POS/pos_test.json\")\n",
    "\n",
    "train_sentences = train_raw[\"sentences\"].tolist()\n",
    "train_tags = train_raw[\"pos_tags\"].tolist()\n",
    "train_tags = [\" \".join(tags) for tags in train_tags]\n",
    "\n",
    "val_sentences = val_raw[\"sentences\"].tolist()\n",
    "val_tags = val_raw[\"pos_tags\"].tolist()\n",
    "val_tags = [\" \".join(tags) for tags in val_tags]\n",
    "\n",
    "test_sentences = test_raw[\"sentences\"].tolist()\n",
    "test_tags = test_raw[\"pos_tags\"].tolist()\n",
    "test_tags = [\" \".join(tags) for tags in test_tags]\n",
    "\n",
    "print(f\"Train: {len(train_raw)} Val: {len(val_raw)} Test: {len(test_raw)}\")\n",
    "i = np.random.randint(0, len(train_raw))\n",
    "print(f\"Random Sample ({i}):\")\n",
    "for token, tag in zip(train_sentences[i].split(\" \"), train_tags[i].split(\" \")):\n",
    "    print(f\"{token:-<20}{tag}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pos_dataset import POSDataset\n",
    "\n",
    "\n",
    "def build_vocab(sequences):\n",
    "    vocab = {}\n",
    "    for seq in sequences:\n",
    "        for word in seq.split(\" \"):\n",
    "            if word not in vocab:\n",
    "                vocab[word] = len(vocab) + 1\n",
    "    vocab[\"PAD\"] = 0\n",
    "    vocab[\"UNK\"] = len(vocab) + 1\n",
    "    return vocab\n",
    "\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "word2idx = build_vocab(train_sentences)\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "tag2idx = build_vocab(train_tags)\n",
    "idx2tag = {idx: tag for tag, idx in tag2idx.items()}\n",
    "max_len = max(len(seq) for seq in train_sentences)\n",
    "\n",
    "train_dataset = POSDataset(train_sentences, train_tags, word2idx, tag2idx, max_len)\n",
    "val_dataset = POSDataset(val_sentences, val_tags, word2idx, tag2idx, max_len)\n",
    "test_dataset = POSDataset(test_sentences, test_tags, word2idx, tag2idx, max_len)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=7,\n",
    "    persistent_workers=True,\n",
    "    generator=sgen,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=7,\n",
    "    persistent_workers=True,\n",
    "    generator=sgen,\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=7,\n",
    "    persistent_workers=True,\n",
    "    generator=sgen,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_batch(dataloader, batch_num):\n",
    "    for b, (words, tags) in enumerate(dataloader):\n",
    "        if b == batch_num:\n",
    "            print(f\"Batch {b}:\")\n",
    "            for sentence, tags in zip(words, tags):\n",
    "                for token, tags in zip(sentence, tags):\n",
    "                    print(f\"{idx2word[int(token)]:<15}{idx2tag[int(tags)]}\")\n",
    "                print(\"=====================================\")\n",
    "            break\n",
    "\n",
    "\n",
    "print_batch(train_loader, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiRNNPOSTagger(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        tag_size,\n",
    "        embedding_dim,\n",
    "        hidden_units,\n",
    "        num_layers=2,\n",
    "        padding_idx=0,\n",
    "        learning_rate=1e-3,\n",
    "        dropout=0.2,\n",
    "    ):\n",
    "        super(BiRNNPOSTagger, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.embedding = torch.nn.Embedding(\n",
    "            vocab_size, embedding_dim, padding_idx=padding_idx\n",
    "        )\n",
    "        self.embedding.weight.data[padding_idx] = torch.zeros(embedding_dim)\n",
    "        self.birnn = torch.nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_units,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.fc = torch.nn.Linear(hidden_units * 2, tag_size)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.training_log = []\n",
    "        self.validation_log = []\n",
    "        self.test_logs = {\"preds\": [], \"targets\": []}\n",
    "        self.last_epoch = 0\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, (_, _) = self.birnn(x)\n",
    "        x = self.fc(self.dropout(x))\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        self.train()\n",
    "        x, y = batch\n",
    "        logits = self.forward(x)\n",
    "        loss = torch.nn.functional.cross_entropy(\n",
    "            logits.view(-1, logits.shape[-1]),\n",
    "            y.view(-1),\n",
    "            ignore_index=self.hparams.padding_idx,\n",
    "        )\n",
    "        self.training_log.append((self.current_epoch, batch_idx, loss.item()))\n",
    "\n",
    "        if self.last_epoch == self.current_epoch:\n",
    "            print(\n",
    "                f\"\\rTraining Epoch {self.current_epoch} Batch {batch_idx} Loss: {loss.item()}     \",\n",
    "                end=\"\",\n",
    "            )\n",
    "        else:\n",
    "            print(\n",
    "                f\"\\nTraining Epoch {self.current_epoch} Batch {batch_idx} Loss: {loss.item()}      \",\n",
    "                end=\"\",\n",
    "            )\n",
    "        self.last_epoch = self.current_epoch\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self.eval()\n",
    "        x, y = batch\n",
    "        logits = self.forward(x)\n",
    "        loss = torch.nn.functional.cross_entropy(\n",
    "            logits.view(-1, logits.shape[-1]),\n",
    "            y.view(-1),\n",
    "            ignore_index=self.hparams.padding_idx,\n",
    "        )\n",
    "        self.validation_log.append((self.current_epoch, batch_idx, loss.item()))\n",
    "\n",
    "        if self.last_epoch == self.current_epoch:\n",
    "            print(\n",
    "                f\"\\rValidation Epoch {self.current_epoch} Batch {batch_idx} Loss: {loss.item()}     \",\n",
    "                end=\"\",\n",
    "            )\n",
    "        else:\n",
    "            print(\n",
    "                f\"\\nValidation Epoch {self.current_epoch} Batch {batch_idx} Loss: {loss.item()}     \",\n",
    "                end=\"\",\n",
    "            )\n",
    "        self.last_epoch = self.current_epoch\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        self.eval()\n",
    "        with torch.inference_mode():\n",
    "            x, y = batch\n",
    "            logits = self.forward(x)\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            self.test_logs[\"preds\"].append(preds.view(-1).cpu().numpy())\n",
    "            self.test_logs[\"targets\"].append(y.view(-1).cpu().numpy())\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPOCHS = 2\n",
    "vocab_size = len(word2idx)\n",
    "tag_size = len(tag2idx)\n",
    "model = BiRNNPOSTagger(vocab_size, tag_size, 8, 2)\n",
    "trainer = pl.Trainer(max_epochs=MAX_EPOCHS, enable_checkpointing=False, logger=False)\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "trainer.test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_epoch_marks = list({log[0] * len(train_loader)\n",
    "                         for log in model.training_log})\n",
    "val_epoch_marks = list({log[0] * len(val_loader)\n",
    "                       for log in model.validation_log})\n",
    "train_loss = [\n",
    "    (log[0] * len(train_loader) + log[1], log[2]) for log in model.training_log\n",
    "]\n",
    "val_loss = [\n",
    "    (log[0] * len(val_loader) + log[1], log[2]) for log in model.validation_log[2:]\n",
    "]\n",
    "\n",
    "fig = px.line(\n",
    "    pd.DataFrame(train_loss, columns=[\"batch\", \"loss\"]),\n",
    "    x=\"batch\",\n",
    "    y=\"loss\",\n",
    "    title=\"Training Loss\",\n",
    "    labels={\"batch\": \"Batch\", \"loss\": \"Loss\"},\n",
    ")\n",
    "for mark in range(MAX_EPOCHS):\n",
    "    fig.add_vline(\n",
    "        x=train_epoch_marks[mark],\n",
    "        line_dash=\"dash\",\n",
    "        line_color=\"red\",\n",
    "        annotation_text=f\"Epoch {mark}\",\n",
    "    )\n",
    "fig.show()\n",
    "\n",
    "fig = px.line(\n",
    "    pd.DataFrame(val_loss, columns=[\"batch\", \"loss\"]),\n",
    "    x=\"batch\",\n",
    "    y=\"loss\",\n",
    "    title=\"Validation Loss\",\n",
    "    labels={\"batch\": \"Batch\", \"loss\": \"Loss\"},\n",
    ")\n",
    "for mark in range(MAX_EPOCHS):\n",
    "    fig.add_vline(\n",
    "        x=val_epoch_marks[mark],\n",
    "        line_dash=\"dash\",\n",
    "        line_color=\"red\",\n",
    "        annotation_text=f\"Epoch {mark}\",\n",
    "    )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "all_preds = model.test_logs[\"preds\"]\n",
    "all_labels = model.test_logs[\"targets\"]\n",
    "\n",
    "filtered_preds = []\n",
    "filtered_labels = []\n",
    "for preds, labels in zip(all_preds, all_labels):\n",
    "    filtered_pred = []\n",
    "    filtered_label = []\n",
    "    for pred, label in zip(preds, labels):\n",
    "        if label == 0 and pred == 0:\n",
    "            continue\n",
    "        filtered_pred.append(pred)\n",
    "        filtered_label.append(label)\n",
    "    filtered_preds.append(filtered_pred)\n",
    "    filtered_labels.append(filtered_label)\n",
    "\n",
    "print(\"Filtered predictions and labels:\")\n",
    "print(f'{\"PRED\":<10}{\"TRUE\":<10}{\"TOKEN\"}')\n",
    "for idx, (pred, labels) in enumerate(zip(filtered_preds, filtered_labels)):\n",
    "    sample = test_raw[\"sentences\"].iloc[idx].split(\" \")\n",
    "    for p, l, s in zip(pred, labels, sample):\n",
    "        print(f\"{idx2tag[p]:<10}{idx2tag[l]:<10}{s}\")\n",
    "    print(\"=======================\")\n",
    "    if idx == 1:\n",
    "        break\n",
    "\n",
    "\n",
    "print(\"Metrics:\")\n",
    "report = classification_report(\n",
    "    np.array(filtered_labels).flatten(),\n",
    "    np.array(filtered_preds).flatten(),\n",
    "    target_names=list(tag2idx.keys()),\n",
    "    labels=list(tag2idx.values()),\n",
    ")\n",
    "print(report)\n",
    "cm = confusion_matrix(filtered_labels, filtered_preds, labels=list(tag2idx.values()))\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
